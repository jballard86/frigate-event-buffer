# Frigate Event Buffer Configuration
# ===================================
#
# Copy this file to config.yaml and update with your values:
#   cp config.example.yaml config.yaml
#
# This file contains non-sensitive application settings.
# Sensitive values can also be set via environment variables,
# which always override values in this file.

# Camera configuration with per-camera label filtering
# Only events from listed cameras will be processed
# Omit a camera entirely to filter it out
cameras:
  # Example: Doorbell - only person and package events
  - name: "YOUR_CAMERA_1"
    labels:
      - "person"
      - "package"

  # Example: Driveway - only vehicle events, with Smart Zone Filtering
  # Only create event when object enters a tracked zone (e.g. driveway); exceptions trigger anywhere
  - name: "YOUR_CAMERA_2"
    labels:
      - "car"
      - "truck"
    event_filters:              # Optional - omit for legacy behavior (all events start immediately)
      tracked_zones:            # Only create event when object enters one of these Frigate zones
        - driveway
      exceptions:               # Labels or sub_labels that create event regardless of zone
        - person
        - UPS
        - FedEx

  # Example: Backyard - allow ALL labels (empty list), no zone filtering
  # - name: "YOUR_CAMERA_3"
  #   labels: []

# Application settings
settings:
  # How many days to keep event recordings
  retention_days: 3

  # How often to run cleanup (in hours)
  cleanup_interval_hours: 1

  # How often to check and remove completed exports from Frigate (minutes)
  export_watchdog_interval_minutes: 2

  # FFmpeg transcode timeout in seconds (kills hung processes)
  ffmpeg_timeout_seconds: 60

  # Max concurrent transcodes for multi-camera consolidated events (overlap download + transcode)
  # default: 2
  # max_concurrent_transcodes: 2

  # Delay before fetching snapshot after initial notification (seconds)
  # Notification is sent instantly; this delay lets Frigate select a better frame
  # before the snapshot update. Set to 0 to fetch immediately.
  notification_delay_seconds: 1

  # Logging level: DEBUG, INFO, WARNING, ERROR
  log_level: "INFO"

  # Review summary time padding (seconds)
  # Controls the time window sent to Frigate's review summary API
  # Covers activity from all GenAI-equipped cameras in the time range
  summary_padding_before: 15   # seconds before event start
  summary_padding_after: 15    # seconds after event end

  # Stats panel auto-refresh interval (seconds)
  # How often the Stats view refreshes when displayed
  stats_refresh_seconds: 60

  # Daily Report (AI-generated from analysis_result.json)
  # How long to keep saved daily reports (days)
  daily_report_retention_days: 90
  # Hour (0-23) to generate AI daily report for previous day
  # daily_report_schedule_hour: 1
  # report_prompt_file: ""
  # report_known_person_name: ""   # Placeholder {known_person_name} in report prompt; default "Unspecified"

  # Frame limits for AI analysis: use multi_cam.max_multi_cam_frames_sec and max_multi_cam_frames_min (all events).
  # Rolling cap: max frames sent to Gemini API per hour; 0 = disabled. Logs rate/blocked each call.
  gemini_frames_per_hour_cap: 200
  # When a consolidated event has one camera, delay (seconds) before closing; 0 = close as soon as event ends.
  # single_camera_ce_close_delay_seconds: 0

  # Consolidated event series
  # Time since last event before the next one starts a NEW consolidated group.
  # If no events for 120s, the next event is new. If an event occurs within
  # this gap, it continues the group and the gap resets.
  event_gap_seconds: 120
  # Events shorter than this (seconds) are discarded: data deleted, removed from state/CE,
  # and a "discarded" MQTT notification is sent so HA can clear the phone notification.
  # minimum_event_seconds: 5
  # Events with duration >= this (seconds) are canceled: no AI/decode, folder renamed with "-canceled",
  # notification "Event canceled see event viewer for details"; default 120 (2 min).
  # max_event_length_seconds: 120
  # Export time range buffers (seconds)
  export_buffer_before: 5
  export_buffer_after: 30

# Network configuration
# These can be overridden by environment variables:
#   MQTT_BROKER, MQTT_PORT, FRIGATE_URL, BUFFER_IP, FLASK_PORT, STORAGE_PATH
network:
  mqtt_broker: "YOUR_MQTT_BROKER_IP"
  mqtt_port: 1883
  # mqtt_user: "your_mqtt_user"
  # mqtt_password: "your_mqtt_password"
  frigate_url: "http://YOUR_FRIGATE_IP:5000"
  # IP/hostname where this buffer container is reachable (used in notification image/video URLs)
  buffer_ip: "YOUR_BUFFER_IP"
  flask_port: 5055
  storage_path: "/app/storage"

# Optional: Gemini proxy (AI analysis) - for clip GenAI summaries
# Single API Key: set GEMINI_API_KEY in environment for production (no separate proxy key).
# No Google Fallback: default proxy URL is ""; set in config or GEMINI_PROXY_URL env.
# gemini:
#   proxy_url: "http://YOUR_PROXY_IP:5050"
#   api_key: ""
#   model: "gemini-2.5-flash-lite"
#   enabled: true

# Optional: Multi-cam frame extractor (main app AI analyzer + standalone multi_cam_recap.py)
# All frame-limit and crop options below are configurable; same config is used by both.
# multi_cam:
#   max_multi_cam_frames_min: 45   # max frames per clip (cap)
#   max_multi_cam_frames_sec: 2    # target capture interval (seconds, float can be a decimal)
#   detection_model: yolov8n.pt   # Ultralytics model for detection sidecar
#   detection_device: ""          # e.g. cuda:0 or cpu; empty = auto
#   motion_threshold_px: 50       # min pixels movement for high-rate capture
#   crop_width: 1280
#   crop_height: 720
#   multi_cam_system_prompt_file: ""   # path to prompt file; empty = use built-in
#   tracking_target_frame_percent: 40   # when person area >= this % of reference area, use full-frame resize
#   smart_crop_padding: 0.15
#   motion_crop_min_area_fraction: 0.001
#   motion_crop_min_px: 500
#   detection_frame_interval: 5   # run YOLO every N frames; increase for more accuracy
#   detection_imgsz: 640          # YOLO inference size; higher = better small objects, slower
#   person_area_debug: false      # draw person area (px²) on frame when true
#
#   # --- Timeline / EMA (Trust-the-EMA pipeline) ---
#   camera_timeline_use_ema_pipeline: true   # true = Phase 1 EMA + hysteresis + merge; false = legacy in-loop selection
#   # Core smoothing & grid
#   camera_timeline_analysis_multiplier: 2   # denser analysis grid vs base step (2 or 3 = smoother curve)
#   camera_timeline_ema_alpha: 0.4           # EMA smoothing 0–1; lower = hold focus during dropouts, higher = faster reaction
#   camera_timeline_primary_bias_multiplier: 1.2   # weight on primary camera area curve
#   # Segment & switching
#   camera_switch_min_segment_frames: 5      # min frames per camera; short runs merged into previous (no drop)
#   camera_switch_hysteresis_margin: 1.15    # new camera must exceed current by this (1.15 = 15%) to switch
#   # Final output
#   camera_timeline_final_yolo_drop_no_person: false   # true = drop no-person frames; false = keep all, tag in metadata


# Optional: Gemini proxy (extended) - model params for proxy calls (e.g. multi-cam)
# API key: GEMINI_API_KEY only. Proxy URL: set here or GEMINI_PROXY_URL env (default "").
# gemini_proxy:
#   url: ""                       # no Google fallback; set explicitly
#   model: "gemini-2.5-flash-lite"
#   temperature: 0.3
#   top_p: 1
#   frequency_penalty: 0
#   presence_penalty: 0

# Optional: Home Assistant REST API (for stats page API Usage display)
# Uncomment and set to show Gemini cost/token usage on the stats page
# ha:
#   base_url: "http://YOUR_HA_IP:8123/api"
#   token: "YOUR_LONG_LIVED_ACCESS_TOKEN"
#   gemini_cost_entity: "input_number.gemini_daily_cost"
#   gemini_tokens_entity: "input_number.gemini_total_tokens"
